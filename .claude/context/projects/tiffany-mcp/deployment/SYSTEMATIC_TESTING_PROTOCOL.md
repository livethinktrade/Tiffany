# SYSTEMATIC TESTING PROTOCOL FOR AGENTS
**Universal Testing Framework for Tiffany MCP Tools**
**Version 1.0 - Multi-Agent Coordination**

---

## üéØ CORE PRINCIPLE: VALIDATE BUSINESS OUTCOMES, NOT JUST API RESPONSES

**NEVER celebrate "success" without validating the actual business logic outcome.**

---

## üìã PRE-TEST REQUIREMENTS

### **STEP 1: EXPECTED OUTCOME DEFINITION**
Before testing ANY tool, the agent MUST:

1. **State the Expected Outcome**: Write exactly what should happen in business terms
   ```
   EXPECTED: For 3 gains, create ONE Airtable record with:
   - Date: Today (Central Time)
   - Gain_1: "First gain description"
   - Gain_2: "Second gain description"
   - Gain_3: "Third gain description"
   - User_Response: Combined summary
   - Mood_Context: "Good"/"Neutral"/"Struggling"
   ```

2. **Define Success Criteria**: List specific, measurable validation points
   ```
   SUCCESS CRITERIA:
   ‚úÖ EXACTLY 1 new record created (not 3 separate records)
   ‚úÖ All 3 gains in Gain_1, Gain_2, Gain_3 fields
   ‚úÖ Date matches today in Central Time Zone
   ‚úÖ Mood_Context uses valid enum value
   ‚úÖ No duplicate records from multiple tool calls
   ```

3. **Identify Failure Modes**: Common ways the tool might "succeed" but be wrong
   ```
   FAILURE MODES TO CHECK:
   ‚ùå Multiple records created instead of one
   ‚ùå Empty Gain_2/Gain_3 fields
   ‚ùå Wrong date/timezone
   ‚ùå Invalid mood values
   ‚ùå API success but no actual data stored
   ```

### **STEP 2: TOOL PREPARATION**
1. **Environment Validation**: Confirm all API keys and credentials
2. **Schema Understanding**: Document the expected data structure
3. **Test Data Preparation**: Create representative test data

---

## üß™ UNIVERSAL TESTING METHODOLOGY

### **PHASE 1: FUNCTIONAL TESTING**

#### **Test 1: Basic Tool Execution**
```javascript
// Template test structure
const testResult = await toolName.execute(testData);

console.log('1. API Response Validation:');
console.log('- Success status:', !testResult.isError);
console.log('- Error messages:', testResult.metadata?.error);
console.log('- Return data:', testResult.metadata);
```

#### **Test 2: Data Persistence Validation**
```javascript
console.log('2. Data Persistence Validation:');
if (testResult.metadata?.airtableId) {
  console.log('‚úÖ Airtable ID returned:', testResult.metadata.airtableId);

  // CRITICAL: Verify the actual data was stored correctly
  await validateAirtableData(testResult.metadata.airtableId);
} else {
  console.log('‚ùå NO AIRTABLE ID - Data likely not persisted!');
}
```

#### **Test 3: Business Logic Validation**
```javascript
console.log('3. Business Logic Validation:');

// For multi-gain storage, verify ONE record with multiple fields
// For quote retrieval, verify actual quote content matches criteria
// For user state, verify context is properly maintained

await validateBusinessLogic(expectedOutcome, actualResult);
```

### **PHASE 2: EDGE CASE TESTING**

#### **Edge Cases to Test**
1. **Boundary Values**: Min/max input sizes, empty strings, null values
2. **Invalid Inputs**: Wrong data types, invalid enum values, malformed data
3. **Network Failures**: API timeouts, connection errors, rate limiting
4. **Concurrent Access**: Multiple agents testing simultaneously
5. **State Dependencies**: Tools that depend on previous data/state

### **PHASE 3: INTEGRATION TESTING**

#### **Workflow Testing**
For tools that work together (like extract_information ‚Üí store_gains):

```javascript
console.log('4. Integration Workflow Testing:');

// Test the complete business workflow
const extractResult = await extractTool.execute(userInput);
const storeResults = [];

for (const gain of extractResult.metadata.extractedData.Gains) {
  const storeResult = await storeTool.execute({
    gain,
    category: 'professional',
    impact: 'large',
    userId: 'test',
    moodContext: extractResult.metadata.extractedData.Mood[0]
  });
  storeResults.push(storeResult);
}

// VALIDATE: Should this create multiple records or one combined record?
await validateWorkflowOutcome(storeResults);
```

---

## üîß VALIDATION TOOLS AGENTS NEED

### **Tool 1: Airtable Data Inspector**
```javascript
// Agents need ability to query and inspect actual Airtable data
async function validateAirtableData(recordId, expectedData) {
  const actualRecord = await fetchAirtableRecord(recordId);

  console.log('Expected vs Actual Comparison:');
  for (const [field, expectedValue] of Object.entries(expectedData)) {
    const actualValue = actualRecord.fields[field];
    const matches = actualValue === expectedValue;
    console.log(`${matches ? '‚úÖ' : '‚ùå'} ${field}: expected "${expectedValue}", got "${actualValue}"`);
  }

  return actualRecord;
}
```

### **Tool 2: Business Logic Validator**
```javascript
// Validate business rules are followed
async function validateBusinessLogic(toolName, testResult, expectedOutcome) {
  const validators = {
    store_gains: validateGainsStorage,
    extract_information: validateInformationExtraction,
    get_random_quote: validateQuoteRetrieval,
    generate_custom_quote: validateQuoteGeneration
  };

  return await validators[toolName](testResult, expectedOutcome);
}
```

### **Tool 3: Multi-Agent Coordination Checker**
```javascript
// Prevent conflicts when multiple agents test simultaneously
async function checkConcurrentTesting(toolName, agentId) {
  const lockFile = `${toolName}_testing.lock`;

  if (await fileExists(lockFile)) {
    const otherAgent = await readFile(lockFile);
    console.log(`‚ö†Ô∏è Tool ${toolName} being tested by ${otherAgent}. Waiting...`);
    await sleep(30000); // Wait 30 seconds
  }

  await writeFile(lockFile, agentId);
}
```

---

## üìä TOOL-SPECIFIC TESTING REQUIREMENTS

### **Data Storage Tools (store_gains, store_user_state)**
```yaml
Expected Outcome:
  - Exactly one new Airtable record created
  - All required fields populated with correct data types
  - Date in Central Time Zone format
  - Enum fields use valid options only

Validation:
  - Query Airtable API to verify record exists
  - Check field values match input data
  - Verify no duplicate records created
  - Confirm timestamps are correct timezone

Success Criteria:
  - Record ID returned in metadata
  - All business rules followed
  - Data visible in Airtable interface
```

### **AI Processing Tools (extract_information, generate_custom_quote)**
```yaml
Expected Outcome:
  - Structured data extraction matches business logic
  - AI-generated content meets quality standards
  - Processing time within acceptable limits
  - Output format matches downstream tool requirements

Validation:
  - Verify extraction accuracy (manual spot checks)
  - Test edge cases (empty input, malformed data)
  - Check integration with dependent tools
  - Validate AI output quality and relevance

Success Criteria:
  - Extraction matches expected structure
  - Generated content is coherent and relevant
  - Integration with other tools works seamlessly
```

### **Retrieval Tools (get_random_quote, get_user_memory)**
```yaml
Expected Outcome:
  - Correct data retrieved based on filters
  - Results match business logic criteria
  - Performance within acceptable range
  - Proper error handling for edge cases

Validation:
  - Verify filter logic works correctly
  - Check result relevance and accuracy
  - Test with empty datasets
  - Validate error handling

Success Criteria:
  - Retrieved data matches filter criteria
  - Performance under 2 seconds
  - Graceful handling of empty results
```

---

## üö® CRITICAL VALIDATION CHECKPOINTS

### **Checkpoint 1: Data Structure Validation**
```javascript
// ALWAYS verify the actual data structure created
if (toolCategory === 'data_storage') {
  const actualRecord = await inspectAirtableRecord(recordId);

  // Compare field by field
  for (const expectedField of expectedFields) {
    if (!actualRecord.fields[expectedField]) {
      throw new Error(`Missing expected field: ${expectedField}`);
    }
  }

  // Verify no unexpected extra records
  const recordCount = await countRecordsCreatedToday();
  if (recordCount !== expectedCount) {
    throw new Error(`Expected ${expectedCount} records, found ${recordCount}`);
  }
}
```

### **Checkpoint 2: Business Rule Compliance**
```javascript
// Verify business logic is followed
const businessRules = {
  gains_storage: {
    rule: 'Multiple gains per day should be stored in single record',
    validator: (result) => result.recordCount === 1 && result.gainsCount > 1
  },
  mood_context: {
    rule: 'Mood must be Good/Neutral/Struggling only',
    validator: (result) => ['Good', 'Neutral', 'Struggling'].includes(result.mood)
  },
  timezone: {
    rule: 'All dates must be Central Time Zone',
    validator: (result) => result.date.includes('America/Chicago')
  }
};

for (const [rule, check] of Object.entries(businessRules)) {
  if (!check.validator(testResult)) {
    throw new Error(`Business rule violation: ${check.rule}`);
  }
}
```

### **Checkpoint 3: Integration Compatibility**
```javascript
// Ensure tool output works with dependent tools
if (hasDownstreamDependencies(toolName)) {
  console.log('Testing downstream integration...');

  const downstreamTools = getDependentTools(toolName);
  for (const dependentTool of downstreamTools) {
    try {
      await testIntegration(testResult.metadata, dependentTool);
      console.log(`‚úÖ Integration with ${dependentTool} successful`);
    } catch (error) {
      console.log(`‚ùå Integration with ${dependentTool} failed: ${error.message}`);
      throw new Error(`Integration failure prevents production readiness`);
    }
  }
}
```

---

## üìã TESTING REPORT TEMPLATE

### **Required Reporting Format**
```markdown
## Tool Test Report: {TOOL_NAME}
**Agent**: {AGENT_ID}
**Date**: {ISO_TIMESTAMP}

### Expected Outcome
[Clear description of what should happen]

### Test Results
**‚úÖ/‚ùå Basic Functionality**: {Pass/Fail + explanation}
**‚úÖ/‚ùå Data Persistence**: {Pass/Fail + record IDs}
**‚úÖ/‚ùå Business Logic**: {Pass/Fail + specific validation}
**‚úÖ/‚ùå Integration**: {Pass/Fail + downstream compatibility}

### Airtable Validation
- Record ID: {AIRTABLE_RECORD_ID}
- Fields Validated: {LIST_OF_FIELDS_CHECKED}
- Business Rules: {COMPLIANCE_STATUS}

### Issues Found
{LIST_ANY_PROBLEMS_OR_EDGE_CASES}

### Production Readiness
**Status**: READY/NOT_READY/NEEDS_WORK
**Blockers**: {ANY_ISSUES_PREVENTING_PRODUCTION_USE}
```

---

## üîÑ AGENT COORDINATION PROTOCOL

### **Before Testing**
1. **Claim Tool**: Update TOOL_TESTING_STATUS.md with agent ID and status
2. **Set Expectations**: Ask user for specific outcome validation if unclear
3. **Prepare Environment**: Verify all dependencies and credentials

### **During Testing**
1. **Follow Protocol**: Execute all validation phases systematically
2. **Document Everything**: Record all findings, even minor issues
3. **Validate Business Logic**: Don't just check API success, verify real outcomes

### **After Testing**
1. **Update Status**: Mark tool as WORKING/FAILED/NEEDS_WORK with detailed report
2. **Share Findings**: Update coordination file for other agents
3. **Report Blockers**: Clearly identify any issues preventing production use

---

## üéØ SUCCESS DEFINITION

**A tool is only "WORKING" when:**
- ‚úÖ All business logic requirements met
- ‚úÖ Data persistence verified in target system
- ‚úÖ Integration with dependent tools confirmed
- ‚úÖ Edge cases handled gracefully
- ‚úÖ Performance meets requirements
- ‚úÖ No data corruption or unintended side effects

**NEVER mark a tool as working based solely on API success responses.**

---

*This protocol ensures consistent, thorough validation across all agents testing Tiffany MCP tools.*